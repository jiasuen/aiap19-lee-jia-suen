# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWU5k2EMvL2gPnxN5KoibMzNS-wGxRmt
"""

import shutil
import os
import sqlite3
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.tree import export_graphviz
import graphviz
from sklearn.ensemble import RandomForestClassifier


from preprocessing import preprocess_data

def prepare_data(plants_df, categorical_columns):
    """Prepares data for both tasks."""
    # Encode categorical columns
    label_encoders = {col: LabelEncoder() for col in categorical_columns}
    for col in categorical_columns:
        plants_df[col] = label_encoders[col].fit_transform(plants_df[col])

    # Task 1 - Predict Temperature
    X_task1 = plants_df.drop(columns=['Temperature Sensor (°C)', 'Plant Type-Stage'])
    y_task1 = plants_df['Temperature Sensor (°C)']

    # Task 2 - Categorize Plant Type-Stage
    X_task2 = plants_df.drop(columns=['Plant Type-Stage'])
    y_task2 = plants_df['Plant Type-Stage']

    # Encode target for Task 2
    label_encoder_target = LabelEncoder()
    y_task2 = label_encoder_target.fit_transform(y_task2)

    # Train-test split
    X_task1_train, X_task1_test, y_task1_train, y_task1_test = train_test_split(X_task1, y_task1, test_size=0.2, random_state=42)
    X_task2_train, X_task2_test, y_task2_train, y_task2_test = train_test_split(X_task2, y_task2, test_size=0.2, random_state=42)

    return (X_task1_train, X_task1_test, y_task1_train, y_task1_test,
            X_task2_train, X_task2_test, y_task2_train, y_task2_test,
            label_encoders, label_encoder_target)

def train_rf_task1(X_train, y_train, X_test, y_test):
    """Trains Random Forest for Task 1."""
    rf = RandomForestRegressor(random_state=42)
    param_grid = {
        'max_depth': [3, 4, 5],
        'min_samples_split': [300, 1000],
        'min_samples_leaf': [300, 1000],
        'max_leaf_nodes': range(5, 15)
    }

    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5,
                               scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_rf = grid_search.best_estimator_

    # Evaluate on test data
    y_pred = best_rf.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mse ** 0.5
    r2 = r2_score(y_test, y_pred)

    print("Random Forest Task 1 Results:")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R² Score: {r2:.4f}")

    return best_rf, mse, rmse, r2

def train_rf_task2(X_train, y_train, X_test, y_test, label_encoder_target):
    """Trains Random Forest for Task 2."""
    rf_classifier = RandomForestClassifier(random_state=42)
    param_grid = {
        'max_depth': [3, 4, 5],
        'min_samples_split': [300, 1000],
        'min_samples_leaf': [300, 1000],
        'max_leaf_nodes': range(5, 15)
    }

    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5,
                               scoring='accuracy', verbose=2, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_rf_classifier = grid_search.best_estimator_

    # Evaluate on test data
    y_pred = best_rf_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=label_encoder_target.classes_)
    matrix = confusion_matrix(y_test, y_pred)

    print("Random Forest Task 2 Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(report)
    print("\nConfusion Matrix:")
    print(matrix)

    return best_rf_classifier, accuracy, report, matrix

def train_nn_task1(X_train, y_train, X_test, y_test):
    """Trains a Neural Network for Task 1."""
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])

    history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),
                        epochs=50, batch_size=32, verbose=1)

    test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)

    print("Neural Network Task 1 Results:")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Mean Absolute Error (MAE): {test_mae:.4f}")

    return model, history, test_loss, test_mae

def train_nn_task2(X_train, y_train, X_test, y_test, num_classes):
    """Trains a Neural Network for Task 2."""
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),
                        epochs=50, batch_size=32, verbose=1)

    test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)

    print("Neural Network Task 2 Results:")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Accuracy: {test_accuracy:.4f}")

    return model, history, test_loss, test_accuracy




# XGBoost for Task 1
def train_xgb_task1(X_train, y_train, X_test, y_test):
    """Trains XGBoost for Task 1."""
    xgb = XGBRegressor(random_state=42, objective='reg:squarederror')
    param_grid = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 4, 5],
        'min_child_weight': [1, 5, 10],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    }

    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5,
                               scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_xgb = grid_search.best_estimator_

    # Evaluate on test data
    y_pred = best_xgb.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mse ** 0.5
    r2 = r2_score(y_test, y_pred)

    return best_xgb, mse, rmse, r2

# XGBoost for Task 2
def train_xgb_task2(X_train, y_train, X_test, y_test, label_encoder_target):
    """Trains XGBoost for Task 2."""
    xgb_classifier = XGBClassifier(
        objective='multi:softmax', num_class=len(label_encoder_target.classes_), random_state=42
    )
    param_grid = {
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [5, 10, 15],
        'n_estimators': [50, 100, 200],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    }

    grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=5,
                               scoring='accuracy', verbose=2, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_xgb = grid_search.best_estimator_

    # Evaluate on test data
    y_pred = best_xgb.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=label_encoder_target.classes_)
    matrix = confusion_matrix(y_test, y_pred)

    return best_xgb, accuracy, report, matrix
